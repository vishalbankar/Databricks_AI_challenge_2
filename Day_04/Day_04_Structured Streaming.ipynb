{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7791d8e0-b3d6-4668-9831-d3870c2f4bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Structured Streaming\n",
    "**`Micro-batch | Checkpointing | Streaming -> Delta`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f02c031-c9a7-4fe7-9e5d-dbec6f436b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What Is Structured Streaming?\n",
    "- Structured Streaming is Spark's built-in stream processing engine. The key insight from the official Databricks docs: although it is called streaming, it works on micro-batch processing. At regular intervals, a batch of new data is ingested, optionally transformed, and written to a sink like a Delta table.\n",
    "\n",
    "The genius of the API is that it looks almost identical to a regular batch DataFrame. The same filter(), groupBy(), join(), withColumn() transformations work on both. `The only difference is readStream instead of read, and writeStream instead of write.`\n",
    "\n",
    "\n",
    "| **Concept**           | **Batch (Days 1-3)**                  | **Structured Streaming **                     |\n",
    "|-----------------------|---------------------------------------|------------------------------------------------------|\n",
    "| **Read**              | spark.read.format('delta').load(path) | spark.readStream.format('delta').table(name)         |\n",
    "| **Transform**         | df.filter(), df.groupBy()             | Same — identical transformation API                  |\n",
    "| **Write**             | df.write.saveAsTable()                | df.writeStream.toTable() or .start()                 |\n",
    "| **Execution**         | Runs once, finishes, stops            | Runs continuously — processes new data as it arrives |\n",
    "| **Trigger**           | No trigger — manual execution         | Micro-batch, fixed interval, or availableNow         |\n",
    "| **Progress tracking** | No tracking needed                    | Checkpoint folder tracks every batch processed       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba64bcf-9a13-431b-a036-a727332bac8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **`From official Databricks docs: Delta Lake is deeply integrated with Spark Structured Streaming through readStream and writeStream. Delta Lake overcomes many limitations typically associated with streaming systems — including coalescing small files from low-latency ingest, maintaining exactly-once processing, and efficiently discovering which files are new.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2387f17e-3cce-4a86-bf82-141eae486a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Micro-batch — How It Actually Works\n",
    "> Micro-batch is the default execution model for Structured Streaming in Spark.   \n",
    "Instead of processing one event at a time (true streaming), Spark collects all new data that arrived since the last batch, processes it together, and writes the result. This repeats on a trigger interval.\n",
    "\n",
    "\n",
    "### MICRO-BATCH CYCLE:\n",
    "\n",
    "> `Trigger fires -> Spark checks source for new data -> Collects new records into one batch\n",
    "-> Applies transformations -> Writes results to sink (Delta table)\n",
    "-> Updates checkpoint -> Sleeps until next trigger -> Repeat`\n",
    "\n",
    "### `Trigger Modes — Four Options`\n",
    "\n",
    "| **Trigger**               | **Code**                              | **Behaviour**                                                                                           | **Best For**                                 |\n",
    "|---------------------------|---------------------------------------|---------------------------------------------------------------------------------------------------------|----------------------------------------------|\n",
    "| **Default (unspecified)** | .writeStream.start()                  | Runs next micro-batch as soon as previous one finishes. No gap.                                         | Maximum throughput — continuous processing   |\n",
    "| **Fixed interval**        | .trigger(processingTime='30 seconds') | Waits 30 seconds between each micro-batch regardless of how fast previous finished.                     | Predictable latency — dashboards, monitoring |\n",
    "| **availableNow**          | .trigger(availableNow=True)           | Processes ALL currently available data then stops. Like a batch job that uses streaming infrastructure. | Daily batch jobs, testing, backfill          |\n",
    "| **Once (deprecated)**     | .trigger(once=True)                   | Processes one micro-batch then stops. Replaced by availableNow.                                         | Legacy — use availableNow instead            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a26334f1-0bc9-4eb7-ade1-e677270462e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checkpointing — The Memory of a Stream\n",
    "A checkpoint is a folder on disk that Structured Streaming writes to after every micro-batch. It records exactly which data has already been processed. When the stream restarts after a failure or notebook restart, it reads the checkpoint and resumes exactly where it left off — not from the beginning.\n",
    "\n",
    "**`Without a checkpoint:`** every time the stream restarts it reprocesses all data from the beginning. This causes duplicate records in your output Delta table.\n",
    "\n",
    "**`With a checkpoint:`** restart is safe. Spark reads the checkpoint, skips already-processed data, and continues from the last committed batch. This gives you exactly-once processing guarantees.\n",
    "\n",
    "\n",
    "**What Is Inside the Checkpoint Folder?**\n",
    "> ```\n",
    "/Volumes/ecommerce/.../checkpoints/events_stream/\n",
    "  commits/          <- one file per completed micro-batch (batch 0, batch 1, batch 2...)\n",
    "  offsets/          <- tracks which source data was included in each batch\n",
    "  metadata          <- stream identity and configuration\n",
    "  state/            <- stateful operation state (for aggregations with watermarks)```\n",
    "\n",
    "\n",
    "**`Critical rule from Databricks docs:`** Every streaming writer must have a UNIQUE checkpoint location. If two streams share the same checkpoint, they corrupt each other's state and you get unpredictable results.\n",
    "\n",
    "**Safe storage location:** Store checkpoints inside your Volume path, not in /tmp/. Volumes are persistent across cluster restarts. /tmp/ is wiped when the cluster terminates — losing your checkpoint means losing stream progress.\n",
    "\n",
    "| Scenario                                  | What Happens                                                                                   |\n",
    "|-------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| First run — no checkpoint exists          | Stream starts from the beginning of the source data. Creates the checkpoint folder.            |\n",
    "| Stream runs successfully                  | After each micro-batch, checkpoint is updated with the batch ID and source offsets.            |\n",
    "| Cluster restarts or notebook re-runs      | Stream reads checkpoint, skips already-processed batches, resumes from last committed offset.  |\n",
    "| Source has no new data                    | Stream sees nothing new in checkpoint vs source. Processes zero records. Checkpoint unchanged. |\n",
    "| You change the stream query significantly | Must delete checkpoint and restart — checkpoint is tied to the specific query structure.       |\n",
    "| You delete the checkpoint manually        | Stream resets completely — reprocesses all source data from the beginning.                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e616a100-3027-484b-8786-1fb54f1e26d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> `For writing events to a Delta table: always use append mode (the default). Complete mode rewrites the entire table every batch — extremely expensive at scale. Update mode requires stateful aggregations with watermarking.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80230d16-c7e8-43b7-a012-e5ff4f9e94c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reading a Stream from a Delta Table\n",
    "Since your ecommerce events are already in a Delta table** (ecommerce.bronze.events from Day 2), you can use that as your streaming source directly. Delta Lake tracks which rows have been added and serves only new ones to each micro-batch.\n",
    "\n",
    "**`How Delta knows what is new:`** Every write to a Delta table creates a new entry in the _delta_log/ transaction log with a version number. The streaming reader tracks which version it last processed in the checkpoint. The next batch reads only the versions it has not seen yet.\n",
    "\n",
    "**`Important constraint:`** Structured Streaming from a Delta table only works if the source table is append-only. If the source table has UPDATE or DELETE operations, use skipChangeCommits=true option (recommended by Databricks for Runtime 12.2+) or handle via Change Data Feed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3dc4902-e89f-43f9-83ae-6e31f2e193f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch vs Streaming — When to Use Which\n",
    "\n",
    "| Factor              | Batch (your Days 1-3)                                         | Structured Streaming (Day 4)                                     |\n",
    "|---------------------|---------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| Data freshness      | Hours or days old — processed on schedule                     | Seconds to minutes — processed as it arrives                     |\n",
    "| Complexity          | Simple — one run, one result                                  | More complex — checkpoint management, trigger tuning             |\n",
    "| Cost                | Lower — cluster runs only during job                          | Higher — cluster runs continuously for true streaming            |\n",
    "| Good for            | Daily feature tables, monthly reports, large historical loads | Real-time dashboards, fraud detection, live recommendation feeds |\n",
    "| Your ecommerce case | Rebuilding user_features_gold daily                           | Ingesting live purchase events to Bronze in near real-time       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41b0341-fad2-4f38-9917-684c103fb05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Practical Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ddf057f-8aa3-44ee-8bea-d344d3fecb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Simulate Streaming from Delta Table**\n",
    "\n",
    "**`What We Are Doing`**\n",
    "> Read from ecommerce.bronze.events_br as a streaming source. We use `readStream instead of read`. The schema must be defined explicitly for streaming reads from files — but since we are reading from a Delta table, Spark infers the schema automatically from the Delta log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446587bb-163a-4f31-96cd-38b82afaa776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:     ecommerce.bronze.events_br\nOutput:     ecommerce.bronze.events_stream\nCheckpoint: /Volumes/ecommerce/sc_ecommerce/vol_ecommerce/checkpoints/events_stream/\n"
     ]
    }
   ],
   "source": [
    "# Define paths (store in variables, never hardcode in stream calls)\n",
    "\n",
    "CATALOG         = 'ecommerce'\n",
    "SOURCE_TABLE    = f'{CATALOG}.bronze.events_br'\n",
    "OUTPUT_TABLE    = f'{CATALOG}.bronze.events_stream'\n",
    "CHECKPOINT_PATH = '/Volumes/ecommerce/sc_ecommerce/vol_ecommerce/checkpoints/events_stream/'\n",
    "\n",
    "print(f'Source:     {SOURCE_TABLE}')\n",
    "print(f'Output:     {OUTPUT_TABLE}')\n",
    "print(f'Checkpoint: {CHECKPOINT_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697d6c63-a2e3-41b7-9601-71b723c2e109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is streaming: True\nSchema:\nroot\n |-- event_time: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- category_id: long (nullable = true)\n |-- category_code: string (nullable = true)\n |-- brand: string (nullable = true)\n |-- price: double (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- user_session: string (nullable = true)\n |-- _ingested_at: timestamp (nullable = true)\n |-- _run_date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Create a streaming DataFrame from the Bronze Delta table\n",
    "# readStream instead of read — this is the ONLY change vs batch reading\n",
    "\n",
    "df_stream = spark.readStream \\\n",
    "    .format('delta') \\\n",
    "    .table(SOURCE_TABLE)\n",
    "\n",
    "# Check that it is a streaming DataFrame\n",
    "print(f'Is streaming: {df_stream.isStreaming}')   # should print: True\n",
    "print(f'Schema:')\n",
    "df_stream.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ef7a89-41e6-417d-bd3a-27a5ca3c919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`df_stream.isStreaming = True` means we now have a streaming DataFrame.\n",
    "\n",
    "we cannot call .count() or .show() on it directly — those are batch actions.\n",
    "> To see data from a streaming DataFrame, you must either:\n",
    "  - (a) write it to a sink with writeStream, OR\n",
    "  - (b) call display(df_stream) in a Databricks notebook — this starts a live stream display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e198ae8c-0acb-4508-9e48-c24b0cd20c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations defined (lazy — nothing has run yet)\nIs still streaming: True\n"
     ]
    }
   ],
   "source": [
    "# lets Apply transformations (same API as batch)\n",
    "# Add a processing timestamp so we know when each record was picked up by the stream\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_transformed = df_stream \\\n",
    "    .withColumn('_stream_processed_at', F.current_timestamp()) \\\n",
    "    .withColumn('event_date', F.to_date(col('event_time'))) \\\n",
    "    .filter(col('event_type').isin(['view', 'cart', 'purchase']))  # same filter as Day 2\n",
    "\n",
    "print('Transformations defined (lazy — nothing has run yet)')\n",
    "print(f'Is still streaming: {df_transformed.isStreaming}')  # still True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c4d75b-5eb5-4b0c-b684-f093bf97daa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2\n",
    "**Write Streaming Output to Delta Table**\n",
    "**`What We Are Doing`**\n",
    "- Write the transformed streaming DataFrame to a new Delta table using writeStream. We use trigger(availableNow=True) so the stream processes all existing data and stops cleanly — no need to manually interrupt it. This is the recommended mode for notebook-based streaming in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5f0816-e05f-4830-b968-4f52fbd42212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream complete!\nOutput written to: ecommerce.bronze.events_stream\n"
     ]
    }
   ],
   "source": [
    "# Write streaming output to Delta using availableNow trigger\n",
    "# availableNow: processes all currently available data then stops automatically\n",
    "\n",
    "query = df_transformed.writeStream \\\n",
    "    .format('delta') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', CHECKPOINT_PATH) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(OUTPUT_TABLE)\n",
    "\n",
    "# Wait for the stream to finish (needed with availableNow)\n",
    "query.awaitTermination()\n",
    "print('Stream complete!')\n",
    "print(f'Output written to: {OUTPUT_TABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d546b408-38b7-4848-ba7f-5fa72e599098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`What each option does:`**\n",
    "\n",
    "`outputMode('append')`     -> only write NEW rows each batch, never modify existing ones   \n",
    "`checkpointLocation `      -> folder where Spark saves batch progress — REQUIRED   \n",
    "`trigger(availableNow=True)` -> process all data now then stop (safe for notebooks)   \n",
    "`toTable(OUTPUT_TABLE) `   -> write to Delta table by name (creates it if not exists)   \n",
    "`awaitTermination() `      -> block the cell until the stream finishes (needed for availableNow)\n",
    "\n",
    "![image_1772163793468.png](./image_1772163793468.png \"image_1772163793468.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6610f74-dcc8-4804-858c-5ffd37898b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|numFiles|sizeInBytes|\n+--------+-----------+\n|      20| 1954959245|\n+--------+-----------+\n\nRecords written to stream output table: 109,950,743\nBronze source records: 109,950,743\n"
     ]
    }
   ],
   "source": [
    "# let' sVerify the output table was created and has data\n",
    "spark.sql(f'DESCRIBE DETAIL {OUTPUT_TABLE}') \\\n",
    "    .select('numFiles', 'sizeInBytes') \\\n",
    "    .show()\n",
    "\n",
    "record_count = spark.table(OUTPUT_TABLE).count()\n",
    "print(f'Records written to stream output table: {record_count:,}')\n",
    "\n",
    "# Should match Bronze count (minus any filtered records)\n",
    "bronze_count = spark.table(SOURCE_TABLE).count()\n",
    "print(f'Bronze source records: {bronze_count:,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d27bb3-835e-4765-a497-7521c8d9ba73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TASK 3 — Simulate New Data Arriving (Streaming Behaviour)\n",
    "**`What We Are Doing`**\n",
    "\n",
    "To see streaming in action — not just a one-time batch — we append new rows to the source Bronze table and re-run the stream. Because the checkpoint remembers what was already processed, the stream picks up ONLY the new rows. This is the core streaming behaviour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32fe8fd9-d0fe-444e-8797-53e26dc9561e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 new events appended to ecommerce.bronze.events_br\nNew Bronze total: 109,950,746\n"
     ]
    }
   ],
   "source": [
    "# Now lte's Append new simulated events to the Bronze source table\n",
    "# This simulates new data arriving from a live source (Kafka, IoT, app events etc.)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),   # \uD83D\uDC48 INT\n",
    "    StructField(\"category_id\", LongType(), True),     # \uD83D\uDC48 MUST be Long (value is huge)\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),      # \uD83D\uDC48 INT\n",
    "    StructField(\"user_session\", StringType(), True),\n",
    "    StructField(\"_ingested_at\", TimestampType(), True),\n",
    "    StructField(\"_run_date\", StringType(), True),\n",
    "])\n",
    "\n",
    "new_events = spark.createDataFrame([\n",
    "    (datetime(2019, 12, 1, 10, 0, 0), 'purchase',\n",
    "     9999001, 2053013555631882655,\n",
    "     'electronics.smartphone', 'samsung',\n",
    "     799.99, 111111111,\n",
    "     'new-session-001', datetime.now(), '2019-12-01'),\n",
    "\n",
    "    (datetime(2019, 12, 1, 10, 1, 0), 'view',\n",
    "     9999002, 2053013555631882655,\n",
    "     'electronics.smartphone', 'apple',\n",
    "     1099.99, 222222222,\n",
    "     'new-session-002', datetime.now(), '2019-12-01'),\n",
    "\n",
    "    (datetime(2019, 12, 1, 10, 2, 0), 'cart',\n",
    "     9999003, 2053013566100866035,\n",
    "     'appliances.kitchen', 'lg',\n",
    "     349.99, 333333333,\n",
    "     'new-session-003', datetime.now(), '2019-12-01'),\n",
    "], schema)\n",
    "\n",
    "\n",
    "# Append to Bronze — this is the new data the stream will pick up\n",
    "new_events.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('append') \\\n",
    "    .saveAsTable(SOURCE_TABLE)\n",
    "\n",
    "print(f'3 new events appended to {SOURCE_TABLE}')\n",
    "print(f'New Bronze total: {spark.table(SOURCE_TABLE).count():,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be1a79d-83aa-4e94-ba41-b61c722f0007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output table now has: 109,950,746 records\nThe difference should be exactly 3 — the new rows only\n"
     ]
    }
   ],
   "source": [
    "# Re-run the stream — it should pick up ONLY the 3 new rows\n",
    "# The checkpoint remembers all previous batches — no reprocessing of old data\n",
    "\n",
    "query2 = df_transformed.writeStream \\\n",
    "    .format('delta') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', CHECKPOINT_PATH) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(OUTPUT_TABLE)\n",
    "\n",
    "query2.awaitTermination()\n",
    "\n",
    "# Verify only the 3 new rows were added\n",
    "new_total = spark.table(OUTPUT_TABLE).count()\n",
    "print(f'Output table now has: {new_total:,} records')\n",
    "print('The difference should be exactly 3 — the new rows only')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e68d9cb-05dd-45a1-adae-8ac1994ca9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**`This is the power of checkpointing. The stream ran twice. First run: processed all existing Bronze data. Second run: processed ONLY the 3 new rows. No duplicates. No reprocessing. This is exactly-once delivery.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b93995-166b-4dfc-a1d1-27b200cea13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TASK 4 — Query Streaming Results\n",
    "`What We Are Doing`\n",
    ">Once data is written to the output Delta table, you query it like any other Delta table using regular batch reads. The streaming part is only for writing — querying is always batch. This is one of the key advantages of Delta Lake as a streaming sink.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52f3de1-8fef-4a93-8568-597b1ede5dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAMING OUTPUT TABLE — OVERVIEW ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_time</th><th>event_type</th><th>product_id</th><th>category_id</th><th>category_code</th><th>brand</th><th>price</th><th>user_id</th><th>user_session</th><th>_ingested_at</th><th>_run_date</th><th>_stream_processed_at</th><th>event_date</th></tr></thead><tbody><tr><td>2019-10-04T08:50:52.000Z</td><td>view</td><td>7901061</td><td>2053013556487520725</td><td>furniture.kitchen.chair</td><td>null</td><td>51.22</td><td>533182637</td><td>a4da7c6b-0c06-4944-bb43-d8f8e86d0b3f</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>1307073</td><td>2053013558920217191</td><td>computers.notebook</td><td>acer</td><td>720.48</td><td>515158111</td><td>39d25f67-79f1-4945-9bdb-5f434505cfc1</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>1700954</td><td>2053013553031414015</td><td>computers.peripherals.monitor</td><td>samsung</td><td>223.68</td><td>514032452</td><td>b6401e78-7207-4846-b8c9-9ba0314a3db0</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>5100182</td><td>2053013553375346967</td><td>null</td><td>moov</td><td>51.41</td><td>531767955</td><td>45720368-8e8a-40c4-b8cd-84e441e3ff49</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>1004872</td><td>2053013555631882655</td><td>electronics.smartphone</td><td>samsung</td><td>292.08</td><td>556652631</td><td>26db4f41-880d-42e9-b443-38bc8fa93008</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>1201216</td><td>2172371436436455782</td><td>electronics.tablet</td><td>lenovo</td><td>253.43</td><td>545382331</td><td>fbdbbf91-9230-4f1f-a95a-69dbf266aed3</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>28101291</td><td>2053013564918072245</td><td>null</td><td>confetti</td><td>7.72</td><td>528253546</td><td>d8f99f9e-b345-4354-98f3-1fe456b4ffcc</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>1004838</td><td>2053013555631882655</td><td>electronics.smartphone</td><td>oppo</td><td>179.28</td><td>512523731</td><td>76c02347-65db-417f-ad06-c9af1cb0bae6</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>3600661</td><td>2053013563810775923</td><td>appliances.kitchen.washer</td><td>samsung</td><td>296.53</td><td>514191743</td><td>ff26f202-cb64-4135-bf01-a9cfb6a9c4fe</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr><tr><td>2019-10-04T08:50:53.000Z</td><td>view</td><td>21400921</td><td>2053013561579406073</td><td>electronics.clocks</td><td>casio</td><td>600.53</td><td>512438486</td><td>ba1cc954-1561-4682-a850-d79ee4018607</td><td>2026-02-26T03:31:01.056Z</td><td>2026-02-25</td><td>2026-02-27T03:38:55.049Z</td><td>2019-10-04</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-10-04T08:50:52.000Z",
         "view",
         7901061,
         2053013556487520725,
         "furniture.kitchen.chair",
         null,
         51.22,
         533182637,
         "a4da7c6b-0c06-4944-bb43-d8f8e86d0b3f",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         1307073,
         2053013558920217191,
         "computers.notebook",
         "acer",
         720.48,
         515158111,
         "39d25f67-79f1-4945-9bdb-5f434505cfc1",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         1700954,
         2053013553031414015,
         "computers.peripherals.monitor",
         "samsung",
         223.68,
         514032452,
         "b6401e78-7207-4846-b8c9-9ba0314a3db0",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         5100182,
         2053013553375346967,
         null,
         "moov",
         51.41,
         531767955,
         "45720368-8e8a-40c4-b8cd-84e441e3ff49",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         1004872,
         2053013555631882655,
         "electronics.smartphone",
         "samsung",
         292.08,
         556652631,
         "26db4f41-880d-42e9-b443-38bc8fa93008",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         1201216,
         2172371436436455782,
         "electronics.tablet",
         "lenovo",
         253.43,
         545382331,
         "fbdbbf91-9230-4f1f-a95a-69dbf266aed3",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         28101291,
         2053013564918072245,
         null,
         "confetti",
         7.72,
         528253546,
         "d8f99f9e-b345-4354-98f3-1fe456b4ffcc",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         1004838,
         2053013555631882655,
         "electronics.smartphone",
         "oppo",
         179.28,
         512523731,
         "76c02347-65db-417f-ad06-c9af1cb0bae6",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         3600661,
         2053013563810775923,
         "appliances.kitchen.washer",
         "samsung",
         296.53,
         514191743,
         "ff26f202-cb64-4135-bf01-a9cfb6a9c4fe",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ],
        [
         "2019-10-04T08:50:53.000Z",
         "view",
         21400921,
         2053013561579406073,
         "electronics.clocks",
         "casio",
         600.53,
         512438486,
         "ba1cc954-1561-4682-a850-d79ee4018607",
         "2026-02-26T03:31:01.056Z",
         "2026-02-25",
         "2026-02-27T03:38:55.049Z",
         "2019-10-04"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "event_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "category_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "category_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "brand",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_session",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_ingested_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "_run_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_stream_processed_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "event_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic query on the streaming output table\n",
    "df_out = spark.table(OUTPUT_TABLE)\n",
    "\n",
    "print('=== STREAMING OUTPUT TABLE — OVERVIEW ===')\n",
    "display(df_out.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc9b563-46ee-4e09-937f-75ffe0b40f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVENT TYPE DISTRIBUTION ===\n+----------+---------+\n|event_type|    count|\n+----------+---------+\n|      view|104335510|\n|      cart|  3955447|\n|  purchase|  1659789|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Event type distribution in streamed data\n",
    "print('=== EVENT TYPE DISTRIBUTION ===')\n",
    "df_out.groupBy('event_type') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23a6c1f-ba2b-41f2-ae86-56372b262b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NEWLY STREAMED ROWS (December 2019) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_time</th><th>event_type</th><th>product_id</th><th>category_id</th><th>category_code</th><th>brand</th><th>price</th><th>user_id</th><th>user_session</th><th>_ingested_at</th><th>_run_date</th><th>_stream_processed_at</th><th>event_date</th></tr></thead><tbody><tr><td>2019-12-01T10:00:00.000Z</td><td>purchase</td><td>9999001</td><td>2053013555631882655</td><td>electronics.smartphone</td><td>samsung</td><td>799.99</td><td>111111111</td><td>new-session-001</td><td>2026-02-27T03:59:24.556Z</td><td>2019-12-01</td><td>2026-02-27T04:01:21.971Z</td><td>2019-12-01</td></tr><tr><td>2019-12-01T10:01:00.000Z</td><td>view</td><td>9999002</td><td>2053013555631882655</td><td>electronics.smartphone</td><td>apple</td><td>1099.99</td><td>222222222</td><td>new-session-002</td><td>2026-02-27T03:59:24.556Z</td><td>2019-12-01</td><td>2026-02-27T04:01:21.971Z</td><td>2019-12-01</td></tr><tr><td>2019-12-01T10:02:00.000Z</td><td>cart</td><td>9999003</td><td>2053013566100866035</td><td>appliances.kitchen</td><td>lg</td><td>349.99</td><td>333333333</td><td>new-session-003</td><td>2026-02-27T03:59:24.556Z</td><td>2019-12-01</td><td>2026-02-27T04:01:21.971Z</td><td>2019-12-01</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2019-12-01T10:00:00.000Z",
         "purchase",
         9999001,
         2053013555631882655,
         "electronics.smartphone",
         "samsung",
         799.99,
         111111111,
         "new-session-001",
         "2026-02-27T03:59:24.556Z",
         "2019-12-01",
         "2026-02-27T04:01:21.971Z",
         "2019-12-01"
        ],
        [
         "2019-12-01T10:01:00.000Z",
         "view",
         9999002,
         2053013555631882655,
         "electronics.smartphone",
         "apple",
         1099.99,
         222222222,
         "new-session-002",
         "2026-02-27T03:59:24.556Z",
         "2019-12-01",
         "2026-02-27T04:01:21.971Z",
         "2019-12-01"
        ],
        [
         "2019-12-01T10:02:00.000Z",
         "cart",
         9999003,
         2053013566100866035,
         "appliances.kitchen",
         "lg",
         349.99,
         333333333,
         "new-session-003",
         "2026-02-27T03:59:24.556Z",
         "2019-12-01",
         "2026-02-27T04:01:21.971Z",
         "2019-12-01"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "event_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "category_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "category_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "brand",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "user_session",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_ingested_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "_run_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_stream_processed_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "event_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the new December rows to confirm they were streamed separately\n",
    "print('=== NEWLY STREAMED ROWS (December 2019) ===')\n",
    "display(\n",
    "    df_out.filter(F.col('event_date') == '2019-12-01')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4bec1ce-2f2f-402e-931b-ab1029345e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STREAM PROCESSING TIMESTAMPS ===\n+----------+-----------------------+-------+\n|event_date|_stream_processed_at   |count  |\n+----------+-----------------------+-------+\n|2019-12-01|2026-02-27 04:01:21.971|3      |\n|2019-11-22|2026-02-27 03:38:55.049|1568243|\n|2019-11-17|2026-02-27 03:38:55.049|6395377|\n|2019-11-25|2026-02-27 03:38:55.049|1593582|\n|2019-11-19|2026-02-27 03:38:55.049|1728541|\n|2019-11-23|2026-02-27 03:38:55.049|1561716|\n|2019-11-20|2026-02-27 03:38:55.049|1700086|\n|2019-11-14|2026-02-27 03:38:55.049|3069726|\n|2019-11-12|2026-02-27 03:38:55.049|1987569|\n|2019-10-30|2026-02-27 03:38:55.049|1210145|\n|2019-10-12|2026-02-27 03:38:55.049|1479896|\n|2019-10-31|2026-02-27 03:38:55.049|1245479|\n|2019-10-13|2026-02-27 03:38:55.049|1639071|\n|2019-11-16|2026-02-27 03:38:55.049|6502957|\n|2019-11-13|2026-02-27 03:38:55.049|2019165|\n|2019-10-29|2026-02-27 03:38:55.049|1227910|\n|2019-10-14|2026-02-27 03:38:55.049|1454702|\n|2019-11-18|2026-02-27 03:38:55.049|2021512|\n|2019-10-15|2026-02-27 03:38:55.049|1542225|\n|2019-11-26|2026-02-27 03:38:55.049|1654879|\n+----------+-----------------------+-------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Check the stream processing timestamps\n",
    "# _stream_processed_at shows WHEN each record was picked up by the stream\n",
    "# You will see two distinct timestamps — one for batch 1, one for batch 2\n",
    "print('=== STREAM PROCESSING TIMESTAMPS ===')\n",
    "df_out.select('event_date', '_stream_processed_at') \\\n",
    "    .groupBy('event_date', '_stream_processed_at') \\\n",
    "    .count() \\\n",
    "    .orderBy('_stream_processed_at', ascending=False) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a1b1a9-c1d1-42d6-bfef-62faafa07747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+------------------------------------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|version|timestamp          |userId          |userName            |operation       |operationParameters                                                                                                                                                                                                                                                                                                                                                                                                                                            |job |notebook         |queryHistoryStatementId             |clusterId               |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                     |userMetadata|engineInfo                                        |\n+-------+-------------------+----------------+--------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+------------------------------------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n|2      |2026-02-27 04:01:25|8815326091183894|bvishaladf@gmail.com|STREAMING UPDATE|{outputMode -> Append, queryId -> 0887d988-a78a-469b-89d2-b5b96ae00aee, epochId -> 1, statsOnLoad -> true}                                                                                                                                                                                                                                                                                                                                                     |NULL|{626798355019702}|bc5c2c30-9e3a-4c93-a699-33c0b6f003ff|0227-033353-43o713un-v2n|1          |WriteSerializable|true         |{numRemovedFiles -> 0, numOutputRows -> 3, numOutputBytes -> 3733, numAddedFiles -> 1}               |NULL        |Databricks-Runtime/18.0.x-aarch64-photon-scala2.13|\n|1      |2026-02-27 03:39:24|8815326091183894|bvishaladf@gmail.com|STREAMING UPDATE|{outputMode -> Append, queryId -> 0887d988-a78a-469b-89d2-b5b96ae00aee, epochId -> 0, statsOnLoad -> true}                                                                                                                                                                                                                                                                                                                                                     |NULL|{626798355019702}|0f1cbf5a-8ecc-43e1-b146-0d8491841dc0|0227-033353-43o713un-v2n|0          |WriteSerializable|true         |{numRemovedFiles -> 0, numOutputRows -> 109950743, numOutputBytes -> 1954959245, numAddedFiles -> 20}|NULL        |Databricks-Runtime/18.0.x-aarch64-photon-scala2.13|\n|0      |2026-02-27 03:38:49|8815326091183894|bvishaladf@gmail.com|CREATE TABLE    |{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.parquet.compression.codec\":\"zstd\",\"delta.enableDeletionVectors\":\"true\",\"delta.enableRowTracking\":\"true\",\"delta.rowTracking.materializedRowCommitVersionColumnName\":\"_row-commit-version-col-be725e45-ec25-43a7-a61e-d793206592ad\",\"delta.rowTracking.materializedRowIdColumnName\":\"_row-id-col-6b643819-7dff-43af-a55a-3d6956cc786c\"}, statsOnLoad -> false}|NULL|{626798355019702}|0f1cbf5a-8ecc-43e1-b146-0d8491841dc0|0227-033353-43o713un-v2n|NULL       |WriteSerializable|true         |{}                                                                                                   |NULL        |Databricks-Runtime/18.0.x-aarch64-photon-scala2.13|\n+-------+-------------------+----------------+--------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+------------------------------------+------------------------+-----------+-----------------+-------------+-----------------------------------------------------------------------------------------------------+------------+--------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use DESCRIBE HISTORY to see each streaming batch as a Delta version\n",
    "# Every micro-batch that wrote - data creates a new version in the Delta log\n",
    "spark.sql(f'DESCRIBE HISTORY {OUTPUT_TABLE}').show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ae8a02-2e58-4a9b-8f7a-4a1262a41c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is still streaming: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Is still streaming: {df_transformed.isStreaming}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003f5406-4295-440b-a611-7e46a92edb53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd78fbb7-a1a5-4ec3-9c79-ceaba8d2fd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Live Stream Display in Notebook (Bonus)\n",
    "In a Databricks notebook, calling `display()` on a streaming DataFrame starts a live streaming job with a real-time progress dashboard. The stream runs continuously until you manually stop it by interrupting the cell. Use this for monitoring and exploration — NOT for production writes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b1f1be-1844-49b2-8b75-1c2ba861d12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/ecommerce/sc_ecommerce/vol_ecommerce/checkpoints/events_stream/\nCheckpointing to /Volumes/ecommerce/sc_ecommerce/vol_ecommerce/checkpoints/events_stream/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6949988642392912>, line 7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(path)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# display(df_transformed)\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m display(df_transformed, checkpointLocation \u001B[38;5;241m=\u001B[39m path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:94\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m     90\u001B[0m         e\n",
       "\u001B[1;32m     91\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     92\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:80\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_streaming_dataframe\u001B[0;34m(self, df, config, streaming_lister, streamName, checkpointLocation, processingTime, trigger, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     78\u001B[0m     query \u001B[38;5;241m=\u001B[39m ws\u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m AnalysisException:\n",
       "\u001B[0;32m---> 80\u001B[0m     query \u001B[38;5;241m=\u001B[39m ws\u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     81\u001B[0m streaming_lister\u001B[38;5;241m.\u001B[39mset_memory_table_name(query\u001B[38;5;241m.\u001B[39mrunId, streamName, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/streaming/readwriter.py:713\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n",
       "\u001B[1;32m    705\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    706\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    711\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    712\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStreamingQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 713\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n",
       "\u001B[1;32m    714\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n",
       "\u001B[1;32m    715\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    716\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n",
       "\u001B[1;32m    717\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n",
       "\u001B[1;32m    718\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n",
       "\u001B[1;32m    719\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n",
       "\u001B[1;32m    720\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n",
       "\u001B[1;32m    721\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/streaming/readwriter.py:675\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    672\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n",
       "\u001B[1;32m    674\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m--> 675\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n",
       "\u001B[1;32m    677\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n",
       "\u001B[1;32m    678\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m    679\u001B[0m )\n",
       "\u001B[1;32m    680\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n",
       "\u001B[1;32m    681\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[1;32m    682\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    686\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    687\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n",
       "\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1591\u001B[0m )\n",
       "\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2139\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2136\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2139\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2140\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2141\u001B[0m     ):\n",
       "\u001B[1;32m   2142\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2143\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2115\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2433\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2435\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2511\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2507\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2509\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2511\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2512\u001B[0m                 info,\n",
       "\u001B[1;32m   2513\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2514\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2515\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2516\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2517\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2520\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2521\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2522\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2523\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unsupportedOutputModeForStreamingOperationError(QueryCompilationErrors.scala:2781)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:307)\n",
       "\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:77)\n",
       "\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:47)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:47)\n",
       "\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:47)\n",
       "\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:437)\n",
       "\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:532)\n",
       "\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:484)\n",
       "\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:389)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:187)\n",
       "\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:72)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4502)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3579)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unsupportedOutputModeForStreamingOperationError(QueryCompilationErrors.scala:2781)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:307)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:77)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:437)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:532)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:484)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:389)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:187)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:72)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3579)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
       },
       "metadata": {
        "errorSummary": "[STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42KDE",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unsupportedOutputModeForStreamingOperationError(QueryCompilationErrors.scala:2781)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:307)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:77)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:437)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:532)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:484)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:389)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:187)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:72)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3579)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6949988642392912>, line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(path)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# display(df_transformed)\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m display(df_transformed, checkpointLocation \u001B[38;5;241m=\u001B[39m path)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:94\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m     90\u001B[0m         e\n\u001B[1;32m     91\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39misStreaming:\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:80\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_streaming_dataframe\u001B[0;34m(self, df, config, streaming_lister, streamName, checkpointLocation, processingTime, trigger, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m     query \u001B[38;5;241m=\u001B[39m ws\u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m AnalysisException:\n\u001B[0;32m---> 80\u001B[0m     query \u001B[38;5;241m=\u001B[39m ws\u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     81\u001B[0m streaming_lister\u001B[38;5;241m.\u001B[39mset_memory_table_name(query\u001B[38;5;241m.\u001B[39mrunId, streamName, config)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/streaming/readwriter.py:713\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    706\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    712\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStreamingQuery\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 713\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_internal(\n\u001B[1;32m    714\u001B[0m         path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m    715\u001B[0m         tableName\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    717\u001B[0m         outputMode\u001B[38;5;241m=\u001B[39moutputMode,\n\u001B[1;32m    718\u001B[0m         partitionBy\u001B[38;5;241m=\u001B[39mpartitionBy,\n\u001B[1;32m    719\u001B[0m         queryName\u001B[38;5;241m=\u001B[39mqueryName,\n\u001B[1;32m    720\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions,\n\u001B[1;32m    721\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/streaming/readwriter.py:675\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_proto\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m tableName\n\u001B[1;32m    674\u001B[0m cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_stream\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m--> 675\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd)\n\u001B[1;32m    677\u001B[0m start_result \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m    678\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mWriteStreamOperationStartResult, properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite_stream_operation_start_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    679\u001B[0m )\n\u001B[1;32m    680\u001B[0m query \u001B[38;5;241m=\u001B[39m StreamingQuery(\n\u001B[1;32m    681\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[1;32m    682\u001B[0m     queryId\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mquery_id\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    686\u001B[0m     name\u001B[38;5;241m=\u001B[39mstart_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    687\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1589\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1587\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_command_in_plan(req\u001B[38;5;241m.\u001B[39mplan, command)\n\u001B[0;32m-> 1589\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1590\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1591\u001B[0m )\n\u001B[1;32m   1592\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1593\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2139\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2136\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2139\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2140\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2141\u001B[0m     ):\n\u001B[1;32m   2142\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2143\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2115\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2114\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2431\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2433\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2435\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2511\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2507\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2509\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2511\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2512\u001B[0m                 info,\n\u001B[1;32m   2513\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2514\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2515\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2516\u001B[0m                 status_code,\n\u001B[1;32m   2517\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2520\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2521\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2522\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2523\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2524\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [STREAMING_OUTPUT_MODE.UNSUPPORTED_OPERATION] Invalid streaming output mode: complete. This output mode is not supported for no streaming aggregations on streaming DataFrames/DataSets. SQLSTATE: 42KDE\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unsupportedOutputModeForStreamingOperationError(QueryCompilationErrors.scala:2781)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:307)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:77)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:142)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:47)\n\tat org.apache.spark.sql.execution.streaming.runtime.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:509)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:663)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:647)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:143)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:509)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:395)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:507)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:506)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:498)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:472)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:619)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:619)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:619)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:365)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:673)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:478)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:646)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:563)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:266)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:412)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:97)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:134)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:623)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:623)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:612)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:569)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:203)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:818)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:169)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:250)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:154)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:1107)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1766)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:1100)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:1097)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:1096)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:1084)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:1095)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:1094)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:560)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:559)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1685)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1746)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:604)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.createQuery(StreamingQueryManager.scala:437)\n\tat org.apache.spark.sql.classic.StreamingQueryManager.startQuery(StreamingQueryManager.scala:532)\n\tat org.apache.spark.sql.classic.streaming.startQuery(DataStreamWriter.scala:484)\n\tat org.apache.spark.sql.classic.streaming.startInternal(DataStreamWriter.scala:389)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:187)\n\tat org.apache.spark.sql.classic.streaming.start(DataStreamWriter.scala:72)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:4502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3579)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:633)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:633)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:632)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:239)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:143)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:55)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:141)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$3(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromParent(DBRTracing.scala:70)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:608)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Live display of the stream in notebook (runs continuously)\n",
    "# WARNING: This starts a continuous stream — interrupt the cell to stop it\n",
    "# Do NOT use this for production writes — use writeStream.toTable() instead\n",
    "path = CHECKPOINT_PATH\n",
    "print(path)\n",
    "# display(df_transformed)\n",
    "display(df_transformed, checkpointLocation = path)\n",
    "\n",
    "# In the notebook UI you will see:\n",
    "# - A live table updating with each micro-batch\n",
    "# - A streaming progress dashboard showing:\n",
    "#     inputRowsPerSecond  — how fast data is arriving\n",
    "#     processedRowsPerSecond — how fast Spark is processing\n",
    "#     batchId — which micro-batch is currently running\n",
    "#     numInputRows — rows in the current batch\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_04_Structured Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}